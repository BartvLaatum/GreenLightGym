parameters:
    policy: 
        value: MlpPolicy
    normalize_advantage:
        value: True
    ent_coef:
        value: 0.01
    vf_coef:
        value: 0.5
    max_grad_norm:
        value: 0.5
    use_sde:
        value: False
    sde_sample_freq:
        value: -1
    target_kl:
        value: None
    optimizer_class:
        value: ADAM
    optimizer_kwargs: 
        value: 
            amsgrad: True
    n_envs:
        value: 12
    std_init:
        distribution: categorical
        values: [0.25, 0.5, 1.0, 2.0]
    n_epochs:
        distribution: categorical
        values: [4, 5, 6, 7, 8, 9, 10, 11, 12]
    n_steps:
        distribution: categorical
        values: [96, 288, 576, 864, 1152, 1440, 2880]
    batch_size:
        distribution: categorical
        values: [32, 64, 128, 256]
    learning_rate:
            distribution: log_uniform_values
            min: 1e-6
            max: 1e-3
    gamma: 
        distribution: uniform
        min: .9
        max: 1.
    gae_lambda: 
        distribution: uniform
        min: .9
        max: 1.
    clip_range:
        distribution: categorical
        values: [.1, .2, .3, .4, .5] #, .35, .4, .45, .5]
    vf_size:
        distribution: categorical
        values: [16, 32, 64, 128, 256, 512, 1028]
    pi_size:
        distribution: categorical
        values: [16, 32, 64, 128, 256, 512, 1028]
    activation_fn:
        distribution: categorical
        values: [Tanh, SiLU, ELU]
    pred_horizon:
        distribution: categorical
        values: [0, 0.004, 0.011, 0.022, 0.044] # 0.25, 0.5, 1]
