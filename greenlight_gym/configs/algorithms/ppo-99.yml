# tuned
15min_four_controls:
  policy: MlpPolicy
#   learning_rate: !!float 2.231e-4
  n_steps: 480      # we update after n_steps calls of step() function. in the case of 10 envs --> 1280 timesteps are taken (equals to 10 days)
  batch_size: 64
  n_epochs: 7 
  gamma: 0.95
  gae_lambda: 0.95
  clip_range: 0.3
  normalize_advantage: True
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: False
  sde_sample_freq: -1
  target_kl: null

  policy_kwargs: {net_arch: {pi: [256, 256], vf: [256, 256]},
                  optimizer_class: ADAM,
                  optimizer_kwargs: {amsgrad: True},
                  activation_fn: ELU,
                  log_std_init: np.log(0.5) # Results in policy standard deviation of 0.5 since exp(log(0.5)) = 0.5
          }

  learning_rate_scheduler: {initial_value: 2.0e-4, final_value: 3.e-5, final_progress: 0.5}

#tuned
5min_four_controls:
  policy: MlpPolicy
  n_steps: 1440      # we update after n_steps calls of step() function. in the case of 10 envs --> 10 * n_steps
  batch_size: 64
  n_epochs: 7 
  gamma: 0.99
  gae_lambda: 0.91
  clip_range: 0.5
  normalize_advantage: True
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: False
  sde_sample_freq: -1
  target_kl: null

  policy_kwargs: {net_arch: {pi: [256, 256], vf: [1024, 1024]},
                  optimizer_class: ADAM,
                  optimizer_kwargs: {amsgrad: True},
                  activation_fn: SiLU,
                  log_std_init: np.log(1) # Results in policy standard deviation of 0.5 since exp(log(0.5)) = 0.5, where np.log(1) results in std of 1
          }

  learning_rate_scheduler: {initial_value: 5.e-4, final_value: 7.e-5, final_progress: 0.5}

5min_pred_hor:
  policy: MlpPolicy
  n_steps: 1440      # we update after n_steps calls of step() function. in the case of 10 envs --> 10 * n_steps
  batch_size: 64
  n_epochs: 7 
  gamma: 0.99
  gae_lambda: 0.91
  clip_range: 0.5
  normalize_advantage: True
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: False
  sde_sample_freq: -1
  target_kl: null

  policy_kwargs: {net_arch: {pi: [256, 256], vf: [1024, 1024]},
                  optimizer_class: ADAM,
                  optimizer_kwargs: {amsgrad: True},
                  activation_fn: SiLU,
                  log_std_init: np.log(1) # Results in policy standard deviation of 0.5 since exp(log(0.5)) = 0.5, where np.log(1) results in std of 1
          }
  learning_rate_scheduler: {initial_value: 5.e-4, final_value: 7.e-5, final_progress: 0.5}

5min_time:
  policy: MlpPolicy
  n_steps: 1440      # we update after n_steps calls of step() function. in the case of 10 envs --> 10 * n_steps
  batch_size: 64
  n_epochs: 7 
  gamma: 0.999
  gae_lambda: 0.995
  clip_range: 0.5
  normalize_advantage: True
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: False
  sde_sample_freq: -1
  target_kl: null

  policy_kwargs: {net_arch: {pi: [512, 512, 512], vf: [1024, 1024, 1024]},
                  optimizer_class: ADAM,
                  optimizer_kwargs: {amsgrad: True},
                  activation_fn: SiLU,
                  log_std_init: np.log(1) # Results in policy standard deviation of 0.5 since exp(log(0.5)) = 0.5, where np.log(1) results in std of 1
          }

  learning_rate: !!float 7.e-6

5min_time_pred_hor:
  policy: MlpPolicy
  n_steps: 1440      # we update after n_steps calls of step() function. in the case of 10 envs --> 10 * n_steps
  batch_size: 64
  n_epochs: 7 
  gamma: 0.995
  gae_lambda: 0.995
  clip_range: 0.5
  normalize_advantage: True
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: False
  sde_sample_freq: -1
  target_kl: null

  policy_kwargs: {net_arch: {pi: [512, 512, 512], vf: [1024, 1024, 1024]},
                  optimizer_class: ADAM,
                  optimizer_kwargs: {amsgrad: True},
                  activation_fn: SiLU,
                  log_std_init: np.log(1) # Results in policy standard deviation of 0.5 since exp(log(0.5)) = 0.5, where np.log(1) results in std of 1
          }
  learning_rate: 2.e-5


5min_time_hor_5hr:
  policy: MlpPolicy
  n_steps: 1440      # we update after n_steps calls of step() function. in the case of 10 envs --> 10 * n_steps
  batch_size: 64
  n_epochs: 7 
  gamma: 0.995
  gae_lambda: 0.995
  clip_range: 0.5
  normalize_advantage: True
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: False
  sde_sample_freq: -1
  target_kl: null

  policy_kwargs: {net_arch: {pi: [512, 512, 512], vf: [1024, 1024, 1024]},
                  optimizer_class: ADAM,
                  optimizer_kwargs: {amsgrad: True},
                  activation_fn: SiLU,
                  log_std_init: np.log(1) # Results in policy standard deviation of 0.5 since exp(log(0.5)) = 0.5, where np.log(1) results in std of 1
          }
  # learning_rate: 2.e-5
  learning_rate_scheduler: {initial_value: 1.e-5, final_value: 5.e-6, final_progress: 0.75}

#tuned
benchmark_mutliplicative_pen:
  policy: MlpPolicy
  n_steps: 1440      # we update after n_steps calls of step() function. in the case of 10 envs --> 10 * n_steps
  batch_size: 64
  n_epochs: 7 
  gamma: 0.995
  gae_lambda: 0.995
  clip_range: 0.5
  normalize_advantage: True
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: False
  sde_sample_freq: -1
  target_kl: null

  policy_kwargs: {net_arch: {pi: [512, 512, 512], vf: [1024, 1024, 1024]},
                  optimizer_class: ADAM,
                  optimizer_kwargs: {amsgrad: True},
                  activation_fn: SiLU,
                  log_std_init: np.log(1) # Results in policy standard deviation of 0.5 since exp(log(0.5)) = 0.5, where np.log(1) results in std of 1
          }

  learning_rate: 7.e-6


multiplicative_pen_daily_avg_temp:
  policy: MlpPolicy
  n_steps: 2880      # we update after n_steps calls of step() function. in the case of 10 envs --> 10 * n_steps
  batch_size: 64
  n_epochs: 7 
  gamma: 0.99
  gae_lambda: 0.995
  clip_range: 0.5
  normalize_advantage: True
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: False
  sde_sample_freq: -1
  target_kl: null

  policy_kwargs: {net_arch: {pi: [512, 512, 512], vf: [1024, 1024, 1024]},
                  optimizer_class: ADAM,
                  optimizer_kwargs: {amsgrad: True},
                  activation_fn: SiLU,
                  log_std_init: np.log(1) # Results in policy standard deviation of 0.5 since exp(log(0.5)) = 0.5, where np.log(1) results in std of 1
          }

  learning_rate: 7.e-6

additive_pen_daily_avg_temp:
  policy: MlpPolicy
  n_steps: 2880      # we update after n_steps calls of step() function. in the case of N envs --> N * n_steps
  batch_size: 64
  n_epochs: 7
  gamma: 0.99
  gae_lambda: 0.995
  clip_range: 0.5
  normalize_advantage: True
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: False
  sde_sample_freq: -1
  target_kl: null

  policy_kwargs: {net_arch: {pi: [512, 512, 512], vf: [1024, 1024, 1024]},
                  optimizer_class: ADAM,
                  optimizer_kwargs: {amsgrad: True},
                  activation_fn: SiLU,
                  log_std_init: np.log(1) # Results in policy standard deviation of 0.5 since exp(log(0.5)) = 0.5, where np.log(1) results in std of 1
          }

  learning_rate: 7.e-6

train_eval_set:
  policy: MlpPolicy
  n_steps: 2880      # we update after n_steps calls of step() function. in the case of 10 envs --> 10 * n_steps
  batch_size: 64
  n_epochs: 7 
  gamma: 0.99
  gae_lambda: 0.995
  clip_range: 0.5
  normalize_advantage: True
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: False
  sde_sample_freq: -1
  target_kl: null

  policy_kwargs: {net_arch: {pi: [512, 512, 512], vf: [1024, 1024, 1024]},
                  optimizer_class: ADAM,
                  optimizer_kwargs: {amsgrad: True},
                  activation_fn: SiLU,
                  log_std_init: np.log(1) # Results in policy standard deviation of 0.5 since exp(log(0.5)) = 0.5, where np.log(1) results in std of 1
          }

  learning_rate: 7.e-6
